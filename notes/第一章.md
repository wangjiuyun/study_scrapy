# 爬虫基础

## HTTP基本原理

### 1.url的基本组成格式

> scheme://[username:password@]hostname[:port][/path][;parameters][?quert][#fragment]

- **其中在[]内部的元素是可以考虑忽略的，因为这个不是必要的，scheme和hostname，port才是主要的**
- scheme：协议，常用的协议有http.https,ftp(scheme也会被称为protocol，两者相同意思)
- username,password:用户名和密码
- hostname：主机地址
- port：端口
- path:路径
- parameters:参数。用来指定访问某个资源时的附加信息
- query:查询，用来查询某类资源，如果有多个查询，则用&隔开
- fragment：片段，他是对资源描述的部分补充，可以理解为资源内部的书签

### 2.HTTP和HTTPS

HTTP的全称是Hypertext Transfer Protocol,中文名为超文本传输协议，其中作用是吧超文本数据从网络传输到本地浏览器，

HTTPS就是加了ssl层，简称HTTPS,因为通过该协议传输的内容都是经过ssl加密的，ssl的主要作用有以下两种

- 建立一个信息安全通道，保证数据传输的安全性
- 确认网站的真实性

| 协议名称    | 主要用途       | 传输层协议           |
| ----------- | -------------- | -------------------- |
| FTP         | 文件传输       | TCP                  |
| TFTP        | 轻量级文件传输 | UDP                  |
| HTTP/HTTPS  | web访问        | TCP                  |
| SMTP        | 邮件发送       | TCP                  |
| IMAP/POP3   | 邮件接收       | TCP                  |
| DNS         | 域名解析       | UDP(查询)，TCP(部分) |
| SSH         | 远程登陆       | TCP                  |
| Telnet      | 远程终端       | TCP                  |
| SNMP        | 网络管理       | UDP                  |
| VOIP(如sip) | 语音通话       | UDP(实时通信)        |
| RTP         | 实时流媒体     | UDP                  |

### 3.HTTP请求过程

在浏览器中输入一个url然后点击搜索就会结果，现在这一整个过程就是，浏览器先向网站所在的服务器发送一个请求，然后网站服务器接收到请求后对其进行处理和解析，然后返回对应的响应，接着传回浏览器，由于响应里包含页面的源代码等内容，所以浏览器在对其进行解析，便将网页呈现出来。

#### 4.请求

​	英文为request，由客户端发向服务器，分为四部分内容，请求方法（request method）,请求网址（request url）,请求头（request headers）,请求体（request body）。

- 请求方法：一般使用get或者post，还有其他的（delete，put）。
- 请求网址。
- 请求头：用来说明服务器要使用的附加信息，比较重要的信息由cookie，referer，user-agent等。
  - accept:请求报头域，用于指定客户端可接受哪些类型的信息。
  - accept-language:用于指定客户端可接受的语言类型。
  - accept-encoding:用于指定客户端可接受的内容编码。
  - host:用于指定请求资源的主机IP和端口号，其内容为请求url的原始服务器或网关的位置。
  - cookie:也常用复数形式cookies，这是网站为了辨别用户，进行绘画跟踪而存储在用户本地的数据。
  - referer:用于标记请求是从那个页面雇来的，服务器可以拿到这个信息做出相应的处理。
  - user-agent:简称UA，这是一个特殊的字符串头，可以是服务器识别客户端使用的操作系统，版本，浏览器以及版本等信息。
  - content-type:也叫互联网媒体类型（internet media type）或者mime类型，在HTTP协议信息头中，它用来表示具体请求中的媒体类型信息。比如texy/html代表html格式，image/gif代表GIF图片，application/json代表JSON类型。
- 请求体：一般承载的内容是post请求中的表单数据，对于get请求，请求体为空。

**content-type和post提交数据方式的关系**

| content-type                      | post提交数据的方式 |
| --------------------------------- | ------------------ |
| application/x-www-form-urlencoded | 表单数据           |
| multipart/form-data               | 表单文件上传       |
| application/json                  | 序列化json数据     |
| text/xml                          | xml数据            |

#### 5.响应

响应即response，由服务器返回给客户端，可以分为三部分，响应状态码（response status code）,响应头（response headers）和响应体（response body）

**简单说明一些常用的响应头信息**

- date:用于标识响应产生的时间
- last-modified:用于指定资源的最后修改时间
- content-encoding:用于指定响应内容的编码
- server：包含服务器的信息，例如名称，版本号等
- content-type:文档类型，指定返回的数据是什么类型
- set-cookie:设置cookie。响应头中的set-cookie用于高数浏览器需要讲此内容放在cookie

**响应体**

就是服务器返回的资源，然后在浏览器的渲染下展示出来。

#### 6.HTTP2.0

http2.0在传输层做了很多优化，主要目标是通过支持完整的请求与响应服用来减少延迟，并通过有效压缩http请求头字段的方式将协议开销至最低，同时增加对请求优先级和服务器推送的支持。

http2.0相较于http1.x,把轻重文本格式修改为二进制分时，使其解析更加高效。同时将请求和响应数据分割为最小的帧，并采用二进制编码

**以下有关于http2.0的概念**

- 帧：只存在于http2.0中的概念，是数据通信的最小单位。比如一个请求被分为了请求头帧和请求体/数据帧
- 数据流：一个虚拟通道，可以承载双向的消息，每个流都有一个唯一的整数ID来标识
- 消息：与逻辑请求或响应消息对应的完整的一系列帧

http2.0将http协议通讯分解为二进制编码帧的交换，这些帧对应着特定数据流中的消息。所有这些都是一个tcp连接内服用，这是HTTP2.0协议所有其他功能和性能优化的基础

**多路复用**

http1.x协议中会对但域名有6~8个tcp连接请求的现在。http2.0不会限制

**控制流**

控制流是一种阻止发送方向接收方发送大量数据的机制，以免超出后者的需求或处理能力。

http2.0提供简单的构建块，这些构建块允许客户端和服务器实现他们自己的数据流和连接级流控制

- 流控制具有方向性
- 流控制的窗口大小是动态调整的
- 流控制无法停用

**服务端推送**

http2.0新增的功能是：服务器可以对一个客户端请求发送多个响应。但是客户端可以选择是否接收，而且主动推送不可以推送第三方资源。

#### http2.0发展现状

主流网站支持http2.0,但是很多网站还是以http1.1为主。

在python中，hyper,httpx支持http2.0，但是requests支持http1.1。

## 1.2web网页基础

#### 1.网页组成

网页可以分为html,css,js

### 2.网页的结构

title是网页标题。html标签内部是head标签和body标签，分别代表网页头和网页体

### 3.节点树及节点间的关系

DOM是W3C(万维网联盟)的标准，英文全称是document object model，即文档对象模型。它定义了html和xml文档的标准。根据w3c的htmldom标准，html文档中的所有内容都是节点

1. 整个网站文档是一个文档节点。
2. 每个html标签对应一个根节点，即上列中的html标签，它属于一个根节点。
3. 节点内的文本时文本节点，比如a节点代表一个超链接，它内部的文本也被认为是一个文本节点。
4. 每个节点的属性是属性节点，比如a节点有一个href属性，它就是一个属性节点。
5. 注释是注释节点，在html中有特殊的语法会被解析为注释，它也会对应一个节点。

### 4.选择器

以id或者class来选定，一下是简单案例看看

| 选择器          | 例子       | 例子描述                     |
| --------------- | ---------- | ---------------------------- |
| .class          | .intro     | 选择class="intro"的所有节点  |
| #id             | #firstname | 选择id='firstname'的所有节点 |
| *               | *          | 选择所有节点                 |
| element         | P          | 选择所有p节点                |
| element,element | div,p      | 选择所有div节点和所有p节点   |
| element element | div p      | 选择div节点内部的所有p节点   |

**还有一个xpath的选择器**

### 5.总结

介绍关于网页的结构和节点间关系



## 1.3  爬虫的基本原理

### 1.爬虫概述

爬虫就是获取网页并提取和保存信息的自动化程序

- **获取网页**
- **提取信息**
- **保存数据**
  - 可以简单保存为txt文本或json文本
  - 保存到mysql和mongdb
- 自动化程序

### 2.能爬怎么样的数据

只要在浏览器里面可以访问到的，就可以抓取下来。只要是http可以请求到的，就能抓取下来

### 3.jS渲染的页面

urllib和requests等库请求当前页面时，只会得到html代码，不会加载js文件，这样也就无法看到完整的页面内容。

这个时候就要用其他的方法

## 1.4 session 和 cookie

### 1.静态网页和动态网页

### 2.无状态HTTP

### 3.Session

session，中文称之为绘画，其本意是指有始有终的一系列动作。

web中，session对象用来存储特定用户session所需的属性及配置信息

### 4.Cookie

cookie，指某些网站为了鉴别用户身份，进行session跟踪而存储在用户本地终端上的数据

**session维持**

#### cookie和session需要配合，一个在客户端，一个在服务端，二者共同协作，就实现登录控制

**属性结构**

f12进入，然后找到application，在查找storage中的cookies，点开查看到下面有很多条目

- name:cookie的名称，cookie一旦创建，名称便不可更改
- value：cookie的值，如果值为unicode字符，则需要为字符编码。如果值为二进制数据，这需要使用base64编码。
- domain:指定可以访问该cookie的域名。例如domain为zhihu.com，则代表所有zhihu.com结尾的域名都可以访问该cookie
- path:cookie的使用路径，如果存在/path/，代表只有该路径才可以使用cookie,如果为/，这代表该域名下面都可以访问
- max-age:cookie的失效时间，单位为秒，常常和expires一起使用，通过此属性可以计算出cookie的有效时间。如果为正数这代表时间之内有效，如果为负数，这代表cookie无效
- size字段：cookie的大小
- http字段：cookie的httponly属性，若此属性为true，则只有在httphrader中才会带有此cookie的信息，而不能通过document.cookie来访问此cookie.
- secure：是否仅允许安全协议传输cookie。

#### 会话cookie和持久cookie

其实没有会话和持久cookie之分，只是max-age或者expires字段决定了cookie失效的时间

### 常见误区

只要关闭浏览器，session就消失。这个不对的，只是服务器对session设置一个时间段，到时间了才会消失。

## 1.5 代理的基本原理

### 1.基本原理

代理实际上就是指代理服务器，英文名叫做proxy server，功能就是代网络用户取得网络信息。客户端可以先把请求发给代理服务器，代理服务器再把请求转发给web服务器，然后web服务器把响应发给代理服务器，代理服务器再把响应发给客户端，其中web服务器识别到的ip是代理服务器而不是真正的客户端。

### 2. 代理的作用

- 突破自身ip的访问限制，访问一些平时不能访问你的站点
- 访问一些单位或者团体的内部资源
- 提高访问速度
- **隐藏自身ip**

### 3.爬虫代理

由于爬虫速度过快，很容易被封，所以采用代理服务器比较好些

### 4.代理分类

1. ftp代理服务器：主要用于访问ftp服务器，一般有上传，下载以及缓存功能。端口为21，2121等
2. http代理服务器：主要用于访问网页，一般具有内容过滤和缓存功能，端口一般为80.8080，3128等
3. ssl/tls代理：主要用于访问加密网站，一般有ssl或tls加密功能，端口一般为443
4. rtsp代理：主要用于realplayer访问real流媒体服务器，一般具有缓存功能，端口一般为554
5. telnet代理，主要用于telnet远程访问，端口一般为23
6. pop3/smtp代理：主要用于pop3/smtp方式送发邮件，一般有缓存功能，端口一般为110、25
7. socks代理：只是丹村传递数据包，不关系具体协议和用法。速度快，一般有缓存功能，端口一般为1080.sock5协议支持tcp和udp，还支持身份验证，服务器端域名解析

**根据匿名程度区分**

- 高度匿名代理：记录IP是代理服务器IP
- 普通匿名代理：
- 透明代理
- 间谍代理：会记录用户数据

**常见代理设置**

- 网上免费
- 有服务商
- adsl拨号



## 1.6 多线程和多进程的基本原理

#### 1. 多线程的含义

进程是线程的集合，进程是由一个或多个现场构成的，线程是操作系统进行运算调度的最小单位，是进程中的最小运行单元。

#### 2 并发和并行

并发是指多个线程对应的多条指令被快速轮换执行

并行是指同一时刻有多条指令在多个处理器上同时执行，这就意味着并行必须依赖多个处理器

### 3 多线程使用场景

如果任务不全是计算密集型任务，就可以使用多线程来提高程序整体的执行效率，尤其对于网络爬虫这种IO密集型任务，使用多线程可以大大提高程序整体的爬取效率。

#### 4 python中的多线程和多进程

python中GIL的限制导致无论是在单核还是多核条件下，同一时刻都只有运行一个线程，这使得python多线程无法发挥多核并行的优势。

GIL全称为global interpreter lock，意思是全局解释器锁，其设计之初是对数据安全的考虑，在python多线程下，每个线程的执行方式会如下三步：

- 获取GIL
- 执行对于线程的代码
- 释放GIL

对于多进程来说，每个进程都有属于自己的gil，所以在多核处理器下，多精彩的运行是不会受到GIL的影响。也就是多进程能够更好地发挥多核优势

python的多进程比多线程要更有优势，所以条件允许的话，尽量使用多进程





